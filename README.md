# WSL2ãƒ»Ubuntuãƒ»Dockerãƒ»Ollamaãƒ»OpenWebUIãƒ»Dify å®Œå…¨æ§‹ç¯‰ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã€RTX 5090å¯¾å¿œç‰ˆã€‘

## ğŸ“‹ ç›®æ¬¡
1. [äº‹å‰æº–å‚™ã¨RTX 5090ã®ç¢ºèª](#1-äº‹å‰æº–å‚™ã¨rtx-5090ã®ç¢ºèª)
2. [WSL2ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨GPUå¯¾å¿œè¨­å®š](#2-wsl2ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨gpuå¯¾å¿œè¨­å®š)
3. [Ubuntuã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨åˆæœŸè¨­å®š](#3-ubuntuã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨åˆæœŸè¨­å®š)
4. [Docker Desktopã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](#4-docker-desktopã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)
5. [CUDA Toolkitã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆRTX 5090å¯¾å¿œï¼‰](#5-cuda-toolkitã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«rtx-5090å¯¾å¿œ)
6. [Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨è¨­å®š](#6-ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨è¨­å®š)
7. [Gemma 3 27Bãƒ¢ãƒ‡ãƒ«ã®å°å…¥](#7-gemma-3-27bãƒ¢ãƒ‡ãƒ«ã®å°å…¥)
8. [OpenWebUIã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨RAGè¨­å®š](#8-openwebuiã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨ragè¨­å®š)
9. [ruri-v3ã®å°å…¥ï¼ˆæ—¥æœ¬èªRAGç”¨ï¼‰](#9-ruri-v3ã®å°å…¥æ—¥æœ¬èªragç”¨)
10. [Difyã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](#10-difyã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)
11. [è‡ªå‹•èµ·å‹•ã®è¨­å®š](#11-è‡ªå‹•èµ·å‹•ã®è¨­å®š)
12. [å‹•ä½œç¢ºèªã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯](#12-å‹•ä½œç¢ºèªã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯)
13. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#13-ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)

---

## 1. äº‹å‰æº–å‚™ã¨RTX 5090ã®ç¢ºèª

### ğŸ–¥ï¸ ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã®ç¢ºèª

**å¿…é ˆè¦ä»¶ï¼š**
- Windows 11 ã¾ãŸã¯ Windows 10 ãƒãƒ¼ã‚¸ãƒ§ãƒ³ 2004ä»¥é™ï¼ˆãƒ“ãƒ«ãƒ‰ 19041ä»¥é™ï¼‰
- **GPUï¼šNVIDIA GeForce RTX 5090**
- ãƒ¡ãƒ¢ãƒªï¼š32GBä»¥ä¸Šï¼ˆæ¨å¥¨ï¼š64GBä»¥ä¸Šï¼‰
- ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼š100GBä»¥ä¸Šã®ç©ºãå®¹é‡
- é›»æºï¼š1000Wä»¥ä¸Šã®PSUï¼ˆRTX 5090ã¯575W TGPï¼‰

### RTX 5090ã®ä»•æ§˜ç¢ºèª
```
- CUDAã‚³ã‚¢æ•°ï¼š21,760
- ãƒ¡ãƒ¢ãƒªï¼š32GB GDDR7
- ãƒ¡ãƒ¢ãƒªå¸¯åŸŸï¼š1,792 GB/s
- Compute Capabilityï¼šsm_120ï¼ˆBlackwellã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰
- CUDA 12.8å¯¾å¿œ
```

### ç¢ºèªæ–¹æ³•

1. **Windowsãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ç¢ºèª**
   ```cmd
   winver
   ```

2. **GPUã®ç¢ºèªï¼ˆPowerShellç®¡ç†è€…æ¨©é™ï¼‰**
   ```powershell
   Get-WmiObject Win32_VideoController | Select-Object Name, DriverVersion
   ```

3. **æœ€æ–°ã®NVIDIAãƒ‰ãƒ©ã‚¤ãƒã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**
   - https://www.nvidia.com/Download/index.aspx ã«ã‚¢ã‚¯ã‚»ã‚¹
   - ã€ŒGeForce RTX 5090ã€ã‚’é¸æŠ
   - Windows 11/10ç”¨ã®Game Readyãƒ‰ãƒ©ã‚¤ãƒï¼ˆ572.16ä»¥é™ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
   - **é‡è¦**ï¼šWSL2ç”¨ã«Linuxãƒ‰ãƒ©ã‚¤ãƒã¯ä¸è¦ï¼ˆWindowsãƒ‰ãƒ©ã‚¤ãƒã‚’å…±æœ‰ï¼‰

---

## 2. WSL2ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨GPUå¯¾å¿œè¨­å®š

### ğŸš€ WSL2ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

1. **PowerShellã‚’ç®¡ç†è€…ã¨ã—ã¦é–‹ã**
   ```powershell
   wsl --install
   ```

2. **PCã‚’å†èµ·å‹•**

3. **WSL2ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«è¨­å®š**
   ```powershell
   wsl --set-default-version 2
   ```

4. **WSLã‚«ãƒ¼ãƒãƒ«ã‚’æœ€æ–°ã«æ›´æ–°**
   ```powershell
   wsl --update
   wsl --shutdown
   ```

### GPUå¯¾å¿œã®ç¢ºèª
```powershell
# WSL2ã§GPUãŒèªè­˜ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
wsl nvidia-smi
```

æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ï¼š
```
+-------------------------------------------------------------------------+
| NVIDIA-SMI 572.16       Driver Version: 572.16       CUDA Version: 12.8 |
|-------------------------------+----------------------+-------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+===================+
|   0  NVIDIA GeForce RTX 5090  Off | 00000000:01:00.0 Off |                  N/A |
```

---

## 3. Ubuntuã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨åˆæœŸè¨­å®š

### ğŸ§ Ubuntu 24.04 LTSã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

1. **Microsoft Storeã‹ã‚‰ Ubuntu 24.04 LTSã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**
   - ã¾ãŸã¯ PowerShellã§ï¼š
   ```powershell
   wsl --install -d Ubuntu-24.04
   ```

2. **åˆå›èµ·å‹•æ™‚ã®è¨­å®š**
   ```bash
   Enter new UNIX username: [ãƒ¦ãƒ¼ã‚¶ãƒ¼å]
   New password: [ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰]
   Retype new password: [ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å†å…¥åŠ›]
   ```

3. **ã‚·ã‚¹ãƒ†ãƒ ã®æ›´æ–°ã¨å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**
   ```bash
   sudo apt update && sudo apt upgrade -y
   sudo apt install -y curl wget git nano build-essential
   ```

4. **GPUã®ç¢ºèªï¼ˆUbuntuå†…ï¼‰**
   ```bash
   nvidia-smi
   ```

---

## 4. Docker Desktopã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### ğŸ³ Docker Desktop for Windowsã®è¨­å®š

1. **Docker Desktopã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**
   - https://www.docker.com/products/docker-desktop/

2. **ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚ã®è¨­å®š**
   - âœ… Use WSL 2 instead of Hyper-V
   - âœ… Add shortcut to desktop

3. **Docker Desktopã®è¨­å®š**
   - Settings â†’ Resources â†’ WSL Integration
   - Ubuntu-24.04ã®ãƒˆã‚°ãƒ«ã‚’ON
   - Apply & Restart

4. **Docker Composeã®ç¢ºèª**
   ```bash
   docker --version
   docker compose version
   ```

---

## 5. CUDA Toolkitã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆRTX 5090å¯¾å¿œï¼‰

### ğŸ”§ CUDA 12.8ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆWSL2å†…ï¼‰

âš ï¸ **é‡è¦**ï¼šWSL2å°‚ç”¨ã®CUDA Toolkitã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆãƒ‰ãƒ©ã‚¤ãƒã¯å«ã¾ã‚Œãªã„ï¼‰

```bash
# CUDA 12.8ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install cuda-toolkit-12-8

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
echo 'export PATH=/usr/local/cuda-12.8/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# ç¢ºèª
nvcc --version
```

---

## 6. Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨è¨­å®š

### ğŸ¤– Ollamaã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

1. **Ollamaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh
   ```

2. **Ollamaã‚µãƒ¼ãƒ“ã‚¹ã®èµ·å‹•**
   ```bash
   sudo systemctl start ollama
   sudo systemctl enable ollama
   ```

3. **GPUä½¿ç”¨ã®æœ€é©åŒ–è¨­å®š**
   ```bash
   # ãƒ¡ãƒ¢ãƒªè¨­å®šï¼ˆRTX 5090ã®32GBã‚’æ´»ç”¨ï¼‰
   export OLLAMA_GPU_MEMORY=30g
   export OLLAMA_MAX_MEMORY=30g
   
   # ~/.bashrcã«è¿½åŠ 
   echo 'export OLLAMA_GPU_MEMORY=30g' >> ~/.bashrc
   echo 'export OLLAMA_MAX_MEMORY=30g' >> ~/.bashrc
   ```

---

## 7. Gemma 3 27Bãƒ¢ãƒ‡ãƒ«ã®å°å…¥

### ğŸ“¥ Gemma 3 27Bã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

1. **ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆé‡å­åŒ–ç‰ˆã‚’é¸æŠï¼‰**
   ```bash
   # 4ãƒ“ãƒƒãƒˆé‡å­åŒ–ç‰ˆï¼ˆç´„17GBã€æ¨å¥¨ï¼‰
   ollama pull gemma3:27b-q4_K_M
   
   # ã¾ãŸã¯8ãƒ“ãƒƒãƒˆé‡å­åŒ–ç‰ˆï¼ˆãƒ¡ãƒ¢ãƒªã«ä½™è£•ãŒã‚ã‚‹å ´åˆï¼‰
   # ollama pull gemma3:27b-q8_0
   ```

2. **ãƒ¢ãƒ‡ãƒ«ã®å‹•ä½œç¢ºèª**
   ```bash
   ollama run gemma3:27b-q4_K_M "ã“ã‚“ã«ã¡ã¯ã€æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚"
   ```

3. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®šã®æœ€é©åŒ–**
   ```bash
   # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’8192ã«è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ2048ã‹ã‚‰æ‹¡å¼µï¼‰
   ollama run gemma3:27b-q4_K_M --num-ctx 8192
   ```

---

## 8. OpenWebUIã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨RAGè¨­å®š

### ğŸŒ Docker Composeã‚’ä½¿ç”¨ã—ãŸOpenWebUIã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

1. **è¨­å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ**
   ```bash
   mkdir -p ~/openwebui && cd ~/openwebui
   ```

2. **docker-compose.ymlã®ä½œæˆ**
   ```yaml
   cat <<EOF > docker-compose.yml
   version: '3.8'
   
   services:
     ollama:
       image: ollama/ollama:latest
       container_name: ollama
       volumes:
         - ollama:/root/.ollama
       ports:
         - "11434:11434"
       restart: unless-stopped
       deploy:
         resources:
           reservations:
             devices:
               - driver: nvidia
                 count: 1
                 capabilities: [gpu]
   
     open-webui:
       image: ghcr.io/open-webui/open-webui:main
       container_name: open-webui
       volumes:
         - open-webui:/app/backend/data
       depends_on:
         - ollama
       ports:
         - "3000:8080"
       environment:
         - OLLAMA_BASE_URL=http://ollama:11434
         - WEBUI_SECRET_KEY=your-secret-key-here
         - ENABLE_RAG_WEB_SEARCH=true
         - RAG_EMBEDDING_ENGINE=ollama
         - CUDA_VISIBLE_DEVICES=0
       restart: unless-stopped
   
   volumes:
     ollama:
     open-webui:
   EOF
   ```

3. **OpenWebUIã®èµ·å‹•**
   ```bash
   docker compose up -d
   ```

4. **åˆå›ã‚¢ã‚¯ã‚»ã‚¹ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**
   - ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://localhost:3000 ã«ã‚¢ã‚¯ã‚»ã‚¹
   - ç®¡ç†è€…ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½œæˆ

---

## 9. ruri-v3ã®å°å…¥ï¼ˆæ—¥æœ¬èªRAGç”¨ï¼‰

### ğŸ—¾ æ—¥æœ¬èªåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š

1. **ruri-v3ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆOllamaç”¨ï¼‰**
   ```bash
   # paraphrase-multilingualã‚’ä»£æ›¿ã¨ã—ã¦ä½¿ç”¨ï¼ˆOllamaå¯¾å¿œï¼‰
   ollama pull paraphrase-multilingual
   ```

2. **OpenWebUIã§ã®RAGè¨­å®š**
   - Settings â†’ Admin Settings â†’ Documents
   - ä»¥ä¸‹ã®è¨­å®šã‚’é©ç”¨ï¼š

   ```
   Content Extraction Engine: Tika
   Text Splitter: Recursive Character
   Chunk Size: 1024
   Chunk Overlap: 100
   Embedding Model Engine: Ollama
   Embedding Model: paraphrase-multilingual:latest
   Full Context Mode: OFF
   Hybrid Search: ON
   Reranking Model: ï¼ˆç©ºæ¬„ï¼‰
   Top K: 8
   Minimum Score: 0.3
   ```

3. **æ—¥æœ¬èªæ–‡æ›¸ã®æœ€é©åŒ–è¨­å®š**
   ```bash
   # OpenWebUIã‚³ãƒ³ãƒ†ãƒŠå†…ã§Tikaã®æ—¥æœ¬èªå¯¾å¿œã‚’ç¢ºèª
   docker exec -it open-webui bash
   apt-get update && apt-get install -y tesseract-ocr-jpn
   exit
   ```

### é«˜åº¦ãªruri-v3è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

**Sentence Transformersã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ**ï¼ˆã‚ˆã‚Šé«˜ç²¾åº¦ï¼‰ï¼š

1. **ã‚«ã‚¹ã‚¿ãƒ Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã®ä½œæˆ**
   ```dockerfile
   cat <<EOF > Dockerfile.custom
   FROM ghcr.io/open-webui/open-webui:main
   
   # PyTorchã®CUDAå¯¾å¿œç‰ˆã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
   RUN pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
   
   # sentence-transformersã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
   RUN pip install sentence-transformers
   
   # æ—¥æœ¬èªå‡¦ç†ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
   RUN pip install fugashi ipadic
   EOF
   ```

2. **docker-compose.ymlã‚’æ›´æ–°**
   ```yaml
   open-webui:
     build:
       context: .
       dockerfile: Dockerfile.custom
   ```

---

## 10. Difyã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### ğŸ¯ Difyã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

1. **Difyã®ã‚¯ãƒ­ãƒ¼ãƒ³ã¨è¨­å®š**
   ```bash
   cd ~
   git clone https://github.com/langgenius/dify.git
   cd dify/docker
   cp .env.example .env
   ```

2. **ç’°å¢ƒå¤‰æ•°ã®ç·¨é›†**
   ```bash
   nano .env
   # ä»¥ä¸‹ã‚’è¿½åŠ /å¤‰æ›´
   # OLLAMA_URL=http://host.docker.internal:11434
   ```

3. **Difyã®èµ·å‹•**
   ```bash
   docker compose up -d
   ```

---

## 11. è‡ªå‹•èµ·å‹•ã®è¨­å®š

### ğŸš€ Windowsèµ·å‹•æ™‚ã®å®Œå…¨è‡ªå‹•åŒ–

1. **èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ**
   ```batch
   mkdir %USERPROFILE%\startup-scripts
   ```

2. **start-ai-services.bat ã®ä½œæˆ**
   ```batch
   @echo off
   echo AI Services Starting...
   
   REM Docker Desktopã‚’èµ·å‹•
   start "" "C:\Program Files\Docker\Docker\Docker Desktop.exe"
   
   REM Dockerèµ·å‹•å¾…æ©Ÿ
   timeout /t 20 /nobreak > nul
   
   REM WSL2ã§ã‚µãƒ¼ãƒ“ã‚¹ã‚’èµ·å‹•
   wsl -d Ubuntu-24.04 -u root bash -c "systemctl start ollama"
   
   REM OpenWebUIã¨Ollamaã‚’èµ·å‹•
   wsl -d Ubuntu-24.04 bash -c "cd ~/openwebui && docker compose up -d"
   
   REM Difyã‚’èµ·å‹•
   wsl -d Ubuntu-24.04 bash -c "cd ~/dify/docker && docker compose up -d"
   
   REM Gemma 3 27Bãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰
   timeout /t 10 /nobreak > nul
   wsl -d Ubuntu-24.04 bash -c "ollama run gemma3:27b-q4_K_M 'ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•ç¢ºèª' > /dev/null 2>&1 &"
   
   echo All AI services started successfully!
   pause
   ```

3. **ã‚¿ã‚¹ã‚¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã¸ã®ç™»éŒ²**
   ```powershell
   $action = New-ScheduledTaskAction -Execute "$env:USERPROFILE\startup-scripts\start-ai-services.bat"
   $trigger = New-ScheduledTaskTrigger -AtLogOn -User $env:USERNAME
   $principal = New-ScheduledTaskPrincipal -UserId $env:USERNAME -RunLevel Highest
   $settings = New-ScheduledTaskSettingsSet -AllowStartIfOnBatteries -DontStopIfGoingOnBatteries
   
   Register-ScheduledTask -TaskName "Start AI Services" -Action $action -Trigger $trigger -Principal $principal -Settings $settings
   ```

---

## 12. å‹•ä½œç¢ºèªã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### âœ… ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ç¢ºèª

1. **GPUä½¿ç”¨çŠ¶æ³ã®ç¢ºèª**
   ```bash
   # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°
   watch -n 1 nvidia-smi
   ```

2. **å„ã‚µãƒ¼ãƒ“ã‚¹ã®çŠ¶æ…‹ç¢ºèª**
   ```bash
   # Ollamaã®ç¢ºèª
   ollama list
   curl http://localhost:11434/api/tags
   
   # Dockerã‚³ãƒ³ãƒ†ãƒŠã®ç¢ºèª
   docker ps
   
   # ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³
   free -h
   ```

3. **Gemma 3 27Bã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**
   ```bash
   # æ¨è«–é€Ÿåº¦ãƒ†ã‚¹ãƒˆ
   time ollama run gemma3:27b-q4_K_M "Explain quantum computing in simple terms"
   ```

4. **RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ**
   - OpenWebUIï¼ˆhttp://localhost:3000ï¼‰ã«ã‚¢ã‚¯ã‚»ã‚¹
   - æ—¥æœ¬èªPDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
   - ã€Œ#ã€ã‚’ä»˜ã‘ã¦æ–‡æ›¸ã‚’å‚ç…§ã—ãªãŒã‚‰è³ªå•

### ğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ

1. **ãƒãƒƒãƒã‚µã‚¤ã‚ºã®èª¿æ•´**
   ```bash
   export OLLAMA_NUM_PARALLEL=4  # ä¸¦åˆ—å‡¦ç†æ•°
   ```

2. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è¨­å®š**
   ```bash
   export OLLAMA_MODELS=/path/to/fast/ssd  # é«˜é€ŸSSDã«ç§»å‹•
   ```

---

## 13. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### â— ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

#### å•é¡Œ1: RTX 5090ãŒèªè­˜ã•ã‚Œãªã„
```bash
# WSL2ã‚«ãƒ¼ãƒãƒ«ã®æ›´æ–°
wsl --update --pre-release

# Windowsãƒ‰ãƒ©ã‚¤ãƒã®å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# DDUï¼ˆDisplay Driver Uninstallerï¼‰ã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```

#### å•é¡Œ2: CUDA 12.8ã‚¨ãƒ©ãƒ¼
```bash
# PyTorchã®CUDA 12.8å¯¾å¿œç¢ºèª
python -c "import torch; print(torch.cuda.is_available())"

# å¿…è¦ã«å¿œã˜ã¦PyTorchã‚’æ›´æ–°
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121
```

#### å•é¡Œ3: ãƒ¡ãƒ¢ãƒªä¸è¶³ï¼ˆ32GB VRAMä½¿ç”¨æ™‚ï¼‰
```bash
# WSL2ã®ãƒ¡ãƒ¢ãƒªåˆ¶é™ã‚’èª¿æ•´
cat <<EOF > /mnt/c/Users/$USER/.wslconfig
[wsl2]
memory=48GB
processors=16
localhostForwarding=true
EOF

wsl --shutdown
```

#### å•é¡Œ4: OpenWebUIã§RAGãŒé…ã„
```yaml
# docker-compose.ymlã«è¿½åŠ 
environment:
  - OLLAMA_FLASH_ATTENTION=1
  - OLLAMA_KV_CACHE_TYPE=q8_0
```

#### å•é¡Œ5: Gemma 3 27Bã®å¿œç­”ãŒé…ã„
```bash
# GPUãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ã‚’æœ€é©åŒ–
export OLLAMA_GPU_LAYERS=62  # RTX 5090ç”¨ã«èª¿æ•´
```

### ğŸ”§ é«˜åº¦ãªãƒ‡ãƒãƒƒã‚°

1. **CUDAã®è©³ç´°æƒ…å ±**
   ```bash
   nvidia-smi -q
   ```

2. **Dockerãƒ­ã‚°ã®ç¢ºèª**
   ```bash
   docker logs -f ollama
   docker logs -f open-webui
   ```

3. **ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®ç›£è¦–**
   ```bash
   htop  # CPU/ãƒ¡ãƒ¢ãƒª
   nvtop  # GPUè©³ç´°
   ```

---

## ğŸ‰ å®Œæˆï¼

ã“ã‚Œã§ã€RTX 5090ã‚’æ´»ç”¨ã—ãŸæœ€å…ˆç«¯ã®AIç’°å¢ƒãŒæ§‹ç¯‰ã§ãã¾ã—ãŸã€‚

### ğŸ“Š æœŸå¾…ã•ã‚Œã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- **Gemma 3 27Bæ¨è«–é€Ÿåº¦**ï¼šç´„30-50 tokens/ç§’ï¼ˆRTX 5090ï¼‰
- **RAGæ¤œç´¢é€Ÿåº¦**ï¼š1ç§’ä»¥å†…ï¼ˆ1000æ–‡æ›¸ï¼‰
- **åŒæ™‚å‡¦ç†èƒ½åŠ›**ï¼š4-8ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
1. ä»–ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®å°å…¥ï¼ˆLlama 3.3 70Bã€DeepSeek-R1ãªã©ï¼‰
2. ã‚«ã‚¹ã‚¿ãƒ RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰
3. Difyã§ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼Gemma 3ã¨ruri-v3ã®é€£æº

### å‚è€ƒãƒªãƒ³ã‚¯
- [NVIDIA CUDA on WSL](https://docs.nvidia.com/cuda/wsl-user-guide/)
- [Ollamaå…¬å¼](https://ollama.com/)
- [OpenWebUI Docs](https://docs.openwebui.com/)
- [ruri-v3è«–æ–‡](https://arxiv.org/abs/2409.07737)
- [Dify Docs](https://docs.dify.ai/)

ã”ä¸æ˜ãªç‚¹ãŒã‚ã‚Œã°ã€å„ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ­ã‚°ã‚’ç¢ºèªã—ã€ä¸Šè¨˜ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚
